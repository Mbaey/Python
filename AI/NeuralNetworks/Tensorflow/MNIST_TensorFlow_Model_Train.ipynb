{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[mnist数据集介绍、读取、保存成图片 C++](https://blog.csdn.net/YF_Li123/article/details/76710028)  \n",
    "[使用MINIST数据集](https://blog.csdn.net/zhaohaibo_/article/details/80634425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "\n",
    "def load_mnist_data(path='MNIST_data/'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    Train_labels_path = os.path.join(path,'%s-labels.idx1-ubyte'% 'train')\n",
    "    Train_images_path = os.path.join(path,'%s-images.idx3-ubyte'% 'train')\n",
    "    Test_labels_path = os.path.join(path,'%s-labels.idx1-ubyte'% 't10k')\n",
    "    Test_images_path = os.path.join(path,'%s-images.idx3-ubyte'% 't10k')\n",
    "    #open(images_path)\n",
    "    print(Train_labels_path)\n",
    "    print(type(Train_labels_path))\n",
    "    with open(Train_labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        Y_train_orig = np.fromfile(lbpath,dtype=np.uint8)\n",
    "    with open(Test_labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "        Y_test_orig = np.fromfile(lbpath,dtype=np.uint8)\n",
    "\n",
    "    with open(Train_images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "        X_train_orig = np.fromfile(imgpath,dtype=np.uint8).reshape(len(Y_train_orig), 784)\n",
    "    with open(Test_images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "        X_test_orig = np.fromfile(imgpath,dtype=np.uint8).reshape(len(Y_test_orig), 784)\n",
    "\n",
    "    return X_train_orig, Y_train_orig, X_test_orig, Y_test_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_data/train-labels.idx1-ubyte\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig  = load_mnist_data()\n",
    "Y_test_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_orig.shape)\n",
    "print(Y_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKElSkgxQkqQkGaAkSUkyQEmSkvR/infCM9we5+sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aca0b44320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=5,\n",
    "    sharex=True,\n",
    "    sharey=True, )\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_test_orig[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False]\n",
      "[]\n",
      "[False False False False  True]\n",
      "[5]\n",
      "[4 1]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([1,2,3,4,5])\n",
    "Y=np.array([5,4,3,2,1])\n",
    "for i in range(2):\n",
    "    print(Y == i)\n",
    "    img = X[Y == i]\n",
    "    print(img)\n",
    "#原来是这样的。。 np.array == 1 后返回真假数组\n",
    "print(Y[[False,True,False,False,True]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_hot_matrix\n",
    "\n",
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C)\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        # Run session and call the output \"result\"\n",
    "        one_hot =  sess.run(one_hot_matrix)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "ind = 3\n",
    "print(Y_test_orig[ind])\n",
    "Y_test_hot = one_hot_matrix(Y_test_orig, 10)\n",
    "print(Y_test_hot.T[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(784, 60000)\n",
      "(10, 60000)\n",
      "(10, 60000)\n",
      "(784, 10000)\n",
      "(10, 10000)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_orig.shape)\n",
    "print(Y_train_orig.shape)\n",
    "############\n",
    "X_train = X_train_orig.T/255\n",
    "Y_train_tensor = tf.one_hot(Y_train_orig, depth=10, axis=0)\n",
    "\n",
    "X_test = X_test_orig.T/255\n",
    "Y_test_tensor = tf.one_hot(Y_test_orig, depth=10, axis=0)\n",
    "#方法1\n",
    "with tf.Session() as sess: \n",
    "    # Run session and call the output \"result\"\n",
    "    Y_train =  sess.run(Y_train_tensor)\n",
    "    Y_test = sess.run(Y_test_tensor)\n",
    "#方法2\n",
    "Y_train2 = np.eye(10)[Y_train_orig.reshape(-1)].T\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_train2.shape)\n",
    "#print(Y_train2)\n",
    "#print(Y_train)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(type(Y_test_tensor))\n",
    "print(type(Y_test))\n",
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "4\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "index=4\n",
    "print(Y_test.T[index])\n",
    "print(Y_test_orig[index])\n",
    "\n",
    "print(Y_train.T[index])\n",
    "print(Y_train_orig[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    # X = tf.placeholder(tf.float32, shape=(n_x,None), name=\"X\")\n",
    "    X = tf.placeholder(tf.float32, (n_x, None))\n",
    "    Y = tf.placeholder(tf.float32, (n_y, None))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable('W' + str(l) , [layer_dims[l], layer_dims[l-1]],  initializer = tf.contrib.layers.xavier_initializer(seed = 1))  #*0.01\n",
    "        parameters['b' + str(l)] = tf.get_variable('b' + str(l),[layer_dims[l], 1],  initializer = tf.zeros_initializer())\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "#     W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "#     W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "#     W3 = tf.get_variable(\"W3\", [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(25, 12288) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters([12288, 25, 12, 6])\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1', '2'])\n"
     ]
    }
   ],
   "source": [
    "diction = {\"1\":12, \"2\":4323}\n",
    "print(diction.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    #print(L)\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    ZL = tf.get_variable('ZL', [1, 1], initializer = tf.zeros_initializer())\n",
    "    for l in range(1, L):\n",
    "        Ztmp = tf.add(tf.matmul( parameters['W' + str(l)],A), parameters['b' + str(l)])\n",
    "        #print('W' + str(l))\n",
    "        #print(parameters['W' + str(l)].shape)\n",
    "        #print(A.shape)    \n",
    "        A =  tf.nn.relu(Ztmp)                                              # A1 = relu(Z1)   \n",
    "        ZL = Ztmp\n",
    "        \n",
    "    ZL =tf.add(tf.matmul( parameters['W' + str(L)],ZL), parameters['b' + str(L)])\n",
    "    return ZL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = Tensor(\"Add_2:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "X, Y = create_placeholders(12288, 6)\n",
    "parameters = initialize_parameters([12288, 25, 12, 6])\n",
    "Z3 = forward_propagation(X, parameters)\n",
    "print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits , labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    assert(X.shape[1] == Y.shape[1])\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "Y = Y_train\n",
    "m = X.shape[1]                  # number of training examples\n",
    "assert(X.shape[1] == Y.shape[1])\n",
    "mini_batches = []\n",
    "np.random.seed(1)\n",
    "print(Y.shape)\n",
    "\n",
    "# Step 1: Shuffle (X, Y)\n",
    "permutation = list(np.random.permutation(m))\n",
    "shuffled_X = X[:, permutation]\n",
    "shuffled_Y = Y[:, permutation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "def model(X_train, Y_train, X_test, Y_test, network_layer=[12288, 25, 12, 10], learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters(network_layer)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(parameters)\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters_NdArray = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        \n",
    "        saver.save(sess, \"./saved_model/model.ckpt\")\n",
    "        print(\"W1 =\")\n",
    "        print(parameters_NdArray[\"W1\"])\n",
    "        print(\"b1 =\")\n",
    "        print(parameters_NdArray[\"b1\"])\n",
    "        print(\"Model saved.\")\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = model(X_train,Y_train, X_test, Y_test, network_layer=[784,16,16,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.075122\n",
      "Cost after epoch 100: 0.101944\n",
      "Cost after epoch 200: 0.074100\n",
      "Cost after epoch 300: 0.058369\n",
      "Cost after epoch 400: 0.047914\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XucXHV9//HXey57SXZz33BJAgFJ\nULyiEaVaixUVrAW1aqFaW2tF+5Ne1NYftj7U6k8f1kut1ktFRdRWFLW1EanUWhBvKAsSNGAkXBPD\nZXPPZrO7M7uf3x/nzGSymZldQk52s+f9fDzmsTPnfM8538OEec/3fM/3O4oIzMzMAArTXQEzM5s5\nHApmZlbnUDAzszqHgpmZ1TkUzMyszqFgZmZ1DgWbFST9l6Q/mu56mB3tHAr2iEi6R9LZ012PiDg3\nIj4/3fUAkHSdpD89AsfplHSZpN2SHpD0pknKvzEttyvdrrNh3UpJ10oakvTLie/pJNu+W9LPJVUl\nvfOwn6gdUQ4Fm/Eklaa7DjUzqS7AO4FVwInAs4G3SDqnWUFJzwcuAZ4DrAROBv6+ocgVwM+AxcDf\nAV+T1DfFbTcCbwG+dVjOyqZXRPjhxyE/gHuAs1useyFwC7AT+BHwhIZ1lwB3AnuA24AXN6z7Y+CH\nwIeB7cD/S5f9APggsAO4Gzi3YZvrgD9t2L5d2ZOA69Nj/w/wceBfW5zDWcBm4P8CDwBfBBYCVwED\n6f6vApan5d8DjAHDwCDwsXT5o4HvpOezAXj5Yfhv/2vgeQ2v3w18uUXZLwHvbXj9HOCB9PlqYATo\nbVj/feD1k2074Rj/Crxzuv9N+vHIHm4pWCYkPRm4DHgdybfPTwFrGy473An8JjCf5Fvnv0o6rmEX\nTwPuApaSfNDWlm0AlgDvBz4rSS2q0K7sl4CfpvV6J/CHk5zOscAikm/kF5G0sD+Xvj4B2Ad8DCAi\n/o7kA/XiiOiJiIslzSUJhC+l53Mh8AlJj212MEmfkLSzxePWtMxC4HhgXcOm64Cm+0yXTyx7jKTF\n6bq7ImJPi32129ZmGYeCZeW1wKci4icRMRbJ9f4R4OkAEfHViNgSEeMR8RXgDuCMhu23RMQ/R0Q1\nIvaly+6NiE9HxBjweeA44JgWx29aVtIJwFOBt0fEaET8AFg7ybmMA++IiJGI2BcR2yLi6xExlH6Q\nvgf4rTbbvxC4JyI+l57PzcDXgZc2KxwR/yciFrR4PCEt1pP+3dWw6S6gt0UdepqUJS0/cd3EfbXb\n1mYZh4Jl5UTgzY3fcoEVJN9ukfQqSbc0rHscybf6mk1N9vlA7UlEDKVPe5qUa1f2eGB7w7JWx2o0\nEBHDtReS5kj6lKR7Je0muRS1QFKxxfYnAk+b8N/iFSQtkEM1mP6d17BsHsklsVblJ5YlLT9x3cR9\ntdvWZhmHgmVlE/CeCd9y50TEFZJOBD4NXAwsjogFwC+AxktBWU3fez+wSNKchmUrJtlmYl3eDJwK\nPC0i5gHPSperRflNwPcm/LfoiYg/a3YwSf8iabDFYz1AROxIz+WJDZs+EVjf4hzWNyn7YERsS9ed\nLKl3wvr1U9jWZhmHgh0OZUldDY8SyYf+6yU9TYm5kn4n/eCZS/LBOQAg6dUkLYXMRcS9QD/wTkkd\nks4Efvdh7qaXpB9hp6RFwDsmrH+Q5A6dmquA1ZL+UFI5fTxV0mNa1PH1aWg0ezT2GXwBeJukhZIe\nTXLJ7vIWdf4C8BpJp6X9EW+rlY2IX5HcEPCO9P17MfAEkktcbbcFSM+ni+TzpJTuo1WryWY4h4Id\nDleTfEjWHu+MiH6SD6mPkdyhs5HkriAi4jbgQ8CPST5AH09yt9GR8grgTGAbyZ1NXyHp75iqfwK6\nga3ADcC3J6z/CPBSSTskfTTtd3gecAGwheTS1j8AnTwy7yDpsL8X+B7wgYj4NoCkE9KWxQkA6fL3\nA9em5e/lwDC7AFhD8l69D3hpRAxMcdtPk7zvF5LczrqPyTvvbYZShH9kx/JN0leAX0bExG/8Zrnj\nloLlTnrp5lGSCulgr/OBb0x3vcxmgpk0OtPsSDkW+HeScQqbgT+LiJ9Nb5XMZgZfPjIzszpfPjIz\ns7qj7vLRkiVLYuXKldNdDTOzo8pNN920NSL6Jit31IXCypUr6e/vn+5qmJkdVSTdO5VyvnxkZmZ1\nDgUzM6tzKJiZWZ1DwczM6hwKZmZW51AwM7M6h4KZmdXlJhRuvGc7H/rvDVTGxqe7KmZmM1ZuQuHm\ne3fwz/+7kdGqQ8HMrJXchEKpmJxqdcwTAJqZtZKbUOgoJj+fO+rLR2ZmLeUmFOothXGHgplZK/kJ\nhULSUvDlIzOz1nITCuW0peC7j8zMWstdKFTH3VIwM2slN6FQqnU0+5ZUM7OWchMK5TQU3FIwM2st\nN6FQKtTGKbilYGbWSn5CIW0pVHz3kZlZS5mFgqTLJD0k6Rct1kvSRyVtlHSrpCdnVReADt99ZGY2\nqSxbCpcD57RZfy6wKn1cBHwyw7p48JqZ2RRkFgoRcT2wvU2R84EvROIGYIGk47KqT23wmi8fmZm1\nNp19CsuATQ2vN6fLMlH2hHhmZpOazlBQk2VNP7ElXSSpX1L/wMDAIR2sVL8l1ZePzMxamc5Q2Ays\naHi9HNjSrGBEXBoRayJiTV9f3yEdrNbR7MFrZmatTWcorAVeld6F9HRgV0Tcn9XBSh68ZmY2qVJW\nO5Z0BXAWsETSZuAdQBkgIv4FuBp4AbARGAJenVVdwIPXzMymIrNQiIgLJ1kfwBuyOv5EZQ9eMzOb\nVI5GNHucgpnZZHITCm4pmJlNLj+hUPA0F2Zmk8lNKBQKoiAPXjMzayc3oQBJv0LFfQpmZi3lKhTK\nBVGpuqVgZtZKvkKhVPDdR2ZmbeQqFEqFgu8+MjNrI1ehUC7KI5rNzNrIVSiUivLcR2ZmbeQqFMrF\nAqNuKZiZtZSvUCgUfPnIzKyNXIVCqSgPXjMzayNnoVCg4j4FM7OWchUK5YLvPjIzaydfoVAseEI8\nM7M2chUKpaI8eM3MrI1chUK56GkuzMzayVUolAq++8jMrJ1chYL7FMzM2stZKLhPwcysnVyFQqno\nEc1mZu3kKhTKRXnwmplZG7kKhZLnPjIzaytfoeA+BTOztnIVCh2++8jMrK1chYJ/ZMfMrL18hUKh\nwNh4EOFgMDNrJlehUC4KwP0KZmYt5CwUktN1v4KZWXOZhoKkcyRtkLRR0iVN1p8g6VpJP5N0q6QX\nZFmfUhoKnv/IzKy5zEJBUhH4OHAucBpwoaTTJhR7G3BlRJwOXAB8Iqv6QMPlI8+UambWVJYthTOA\njRFxV0SMAl8Gzp9QJoB56fP5wJYM60Op4JaCmVk7WYbCMmBTw+vN6bJG7wReKWkzcDXw5812JOki\nSf2S+gcGBg65QqV6R7NbCmZmzWQZCmqybOJX9AuByyNiOfAC4IuSDqpTRFwaEWsiYk1fX98hV6jD\nHc1mZm1lGQqbgRUNr5dz8OWh1wBXAkTEj4EuYElWFaq1FDyAzcysuSxD4UZglaSTJHWQdCSvnVDm\nPuA5AJIeQxIKh359aBK1PgW3FMzMmsssFCKiClwMXAPcTnKX0XpJ75J0XlrszcBrJa0DrgD+ODIc\nbly7+8gdzWZmzZWy3HlEXE3Sgdy47O0Nz28DnpFlHRqV3KdgZtZWzkY0e5oLM7N2chYK6TgFD14z\nM2sqV6FQKrhPwcysnVyFgifEMzNrL1ehUHKfgplZW7kKBfcpmJm1l69QqA9ec0vBzKyZXIVCfZoL\n9ymYmTWVy1CoeO4jM7OmchUK9ctHVbcUzMyayVcolNzRbGbWTq5CoTZ4zR3NZmbN5SoU6rekOhTM\nzJrKVSgUC0Ly5SMzs1ZyFQqQtBZGfUuqmVlT+QuFgnz5yMyshdyFQqlY8OA1M7MWchcK5aI8eM3M\nrIXchUKpUPDgNTOzFnIXCuWSqLqlYGbWVP5CoVDwj+yYmbWQu1AoFX33kZlZK/kLhULBg9fMzFrI\nXSiUi2LULQUzs6ZyGAoep2Bm1kruQsF9CmZmreUuFMrFAhX3KZiZNZW7UCh57iMzs5byFwpFj1Mw\nM2sl01CQdI6kDZI2SrqkRZmXS7pN0npJX8qyPgAdDgUzs5ZKWe1YUhH4OPBcYDNwo6S1EXFbQ5lV\nwFuBZ0TEDklLs6pPTanoaS7MzFrJsqVwBrAxIu6KiFHgy8D5E8q8Fvh4ROwAiIiHMqwPkA5ec5+C\nmVlTUwoFSS+byrIJlgGbGl5vTpc1Wg2slvRDSTdIOqfF8S+S1C+pf2BgYCpVbikZvObLR2ZmzUy1\npfDWKS5rpCbLJn5FLwGrgLOAC4HPSFpw0EYRl0bEmohY09fXN4XqtpaMU3AomJk107ZPQdK5wAuA\nZZI+2rBqHlCdZN+bgRUNr5cDW5qUuSEiKsDdkjaQhMSNU6j7IUlGNPvykZlZM5O1FLYA/cAwcFPD\nYy3w/Em2vRFYJekkSR3ABel2jb4BPBtA0hKSy0l3PZwTeLg8eM3MrLW2LYWIWAesk/Sl9Ns8khYC\nK2qdw222rUq6GLgGKAKXRcR6Se8C+iNibbrueZJuA8aAv4mIbY/8tFrz4DUzs9amekvqdySdl5a/\nBRiQ9L2IeFO7jSLiauDqCcve3vA8gDeljyOiVCxQHQ8iAqlZt4eZWX5NtaN5fkTsBl4CfC4ingKc\nnV21stNRTIKg4taCmdlBphoKJUnHAS8HrsqwPpkrFZNT9g/tmJkdbKqh8C6S6/93RsSNkk4G7siu\nWtkpFdxSMDNrZUp9ChHxVeCrDa/vAn4vq0plqVxrKXisgpnZQaY6onm5pP+Q9JCkByV9XdLyrCuX\nhZL7FMzMWprq5aPPkYwxOJ5kqopvpsuOOrWWgmdKNTM72FRDoS8iPhcR1fRxOfDI5puYJuW0peCZ\nUs3MDjbVUNgq6ZWSiunjlUCmg8yyUiq4T8HMrJWphsKfkNyO+gBwP/BS4NVZVSpLZfcpmJm1NNUR\nze8G/qg2tYWkRcAHScLiqFJrKbhPwczsYFNtKTyhca6jiNgOnJ5NlbJVLnnwmplZK1MNhUI6ER5Q\nbylk9lOeWSp78JqZWUtT/WD/EPAjSV8j+aGclwPvyaxWGapPc+FQMDM7yFRHNH9BUj/w2yS/qPaS\niLgt05plZP/gNV8+MjObaMqXgNIQOCqDoFHZHc1mZi1NtU9h1iiXPHjNzKyV3IWCb0k1M2std6FQ\nn+bCHc1mZgfJXSiUPCGemVlLuQuF+jgF9ymYmR0kf6HgH9kxM2spd6FQcp+CmVlLuQuF+o/seO4j\nM7OD5C4USrU+hapbCmZmE+UuFIoFIXmWVDOzZnIXCpIoFwqeJdXMrInchQIknc2++8jM7GD5DIWC\nPHjNzKyJXIZCuVjw4DUzsyYyDQVJ50jaIGmjpEvalHuppJC0Jsv61JSLBV8+MjNrIrNQkFQEPg6c\nC5wGXCjptCbleoG/AH6SVV0mSvoU3FIwM5soy5bCGcDGiLgrIkaBLwPnNyn3buD9wHCGdTmALx+Z\nmTWXZSgsAzY1vN6cLquTdDqwIiKuarcjSRdJ6pfUPzAw8IgrViqIStWXj8zMJsoyFNRkWf3ruaQC\n8GHgzZPtKCIujYg1EbGmr6/vEVesVCx48JqZWRNZhsJmYEXD6+XAlobXvcDjgOsk3QM8HVh7JDqb\nO4ry4DUzsyayDIUbgVWSTpLUAVwArK2tjIhdEbEkIlZGxErgBuC8iOjPsE6AWwpmZq1kFgoRUQUu\nBq4BbgeujIj1kt4l6bysjjsVyeA1txTMzCYqZbnziLgauHrCsre3KHtWlnVpVC4W2DtaPVKHMzM7\nauRyRLPHKZiZNZfLUCgXC577yMysiZyGgqh68JqZ2UFyGQqlguc+MjNrJp+h4HEKZmZN5TIUkl9e\nc0vBzGyifIZCyX0KZmbN5DIUSm4pmJk1lctQKBf9c5xmZs3kMhRKxYIHr5mZNZHLUCgXC1THgwgH\ng5lZo3yGQiH5qQd3NpuZHSiXoVAqJqftS0hmZgfKZSiUi0lLYdSdzWZmB8hlKJRql48cCmZmB8hl\nKJRL6eUj9ymYmR0gl6HQ21UG4KHdI9NcEzOzmSWXoXDmyYsBuP6OgWmuiZnZzJLLUOjr7eTxy+Zz\n7S8fmu6qmJnNKLkMBYCzTu3j5vt2sGuoMt1VMTObMXIcCksZD19CMjNrlNtQeNKKBSyYU+a6DQ4F\nM7Oa3IZCsSB+c1Uf3/vVAOO+NdXMDMhxKAA8+9Q+tg6OsH7L7umuipnZjJDrUHjW6j4Artvgu5DM\nzCDnobCkp5MnLp/PtQ4FMzMg56EA8FunLuWWTTvZNujRzWZmuQ+F33n8cQRw6fV3TXdVzMymXe5D\n4dRje3nJ6cv53A/vYdP2oemujpnZtMo0FCSdI2mDpI2SLmmy/k2SbpN0q6TvSjoxy/q08tfPX40E\nH7hmw3Qc3sxsxsgsFCQVgY8D5wKnARdKOm1CsZ8BayLiCcDXgPdnVZ92jpvfzWt/82TWrtvCLZt2\nTkcVzMxmhCxbCmcAGyPirogYBb4MnN9YICKujYjaNZsbgOUZ1qet15/1KJb0dPDeb91OhAezmVk+\nZRkKy4BNDa83p8taeQ3wX81WSLpIUr+k/oGBbKal6Oks8cbnruan92znsh/ek8kxzMxmuixDQU2W\nNf0KLumVwBrgA83WR8SlEbEmItb09fUdxioe6IKnnsA5jz2Wd191G2vXbcnsOGZmM1WWobAZWNHw\nejlw0CetpLOBvwPOi4hpHSxQLIh/uuBJnHHSIt585S384I6t01kdM7MjLstQuBFYJekkSR3ABcDa\nxgKSTgc+RRIIM2JYcVe5yKdftYZH9fXwui/28+M7t013lczMjpjMQiEiqsDFwDXA7cCVEbFe0rsk\nnZcW+wDQA3xV0i2S1rbY3RE1v7vM5//kDI6d38UrP/sTPvuDu935bGa5oKPtw27NmjXR399/RI61\nZ7jCX391Hdesf5Dzn3Q8733x45nbWToixzYzO5wk3RQRayYrl/sRze30dpX55Cuewt88/1TWrtvC\n2f/4Pb516/1uNZjZrOVQmEShIN7w7FP46uvOZMGcDt7wpZv5w8/+lNvv928wmNns41CYojUrF/HN\ni5/B35/3WNZt3sm5H/k+F32hn1/8etd0V83M7LBxn8Ih2DVU4bIf3s1lP7ybPcNVzjx5MRecsYLn\nP/ZYusrFaa2bmVkzU+1TcCg8AruHK3zxx/dyxU/vY/OOfSyYU+ZFT1rGS568jMcvm4/UbPyemdmR\n51A4gsbHgx/duY0rbryP76x/kNGxcVYt7eFFpy/j+Y89llOW9kx3Fc0s5xwK02TXUIWrfr6Fr9+0\nmZvvS2ZcPWVpD8897RjOWt3Hk09cSLnorhwzO7IcCjPA/bv28d/rH+Tbv3iAG+/ZTnU86O0s8Run\nLOZZq/t41qo+ViyaM93VNLMccCjMMLuHK/xo41au2zDA9b8aYMuuYQBOXDyHp5y4kDUnLuIpJy7k\nlKU9FAvuizCzw8uhMINFBHcO7OX7dwzw4zu3cdO9O9i2dxSAuR1FHrdsPk9csYDHHj+P046bx8l9\nDgoze2QcCkeRiOCebUPcfO8Obt28k3Wbd3Hb/bsZrY4D0FUusPqYXh59bC+nHjuPU4/pZfUxPfT1\ndvoOJzObEofCUa4yNs6dA4PctmU367fsZsMDe7j9/t31FgXAgjllTunr4VF9PZyytIeTlsxl5ZK5\nnLBoDh0ld2ab2X4OhVlqYM8Idzy4h189uIcNDw5y58Agdz40eEBYFATHL+jmhEVzksfi5O+Ji+ay\nYlE387vLbmGY5cxUQ8FTfh5l+no76evt5DdOWXLA8h17R7l7217u2Zo87t0+xH3bh/jObQ8eEBiQ\n9Fscv6CbZQu7Wb6wm+UL57BsQTfHL+jiuPndLO3tpOTbZs1yyaEwSyyc28HCuR08+YSFB60bHKly\n37Yh7tu+l8079rFl5zC/3jnEr3fu45ZNO9k5VDmgvASL53bQ19vF0t5OlqZBdMy8Lo6d38Vx85O/\ni+d2ugPcbJZxKORAT2eJ046fx2nHz2u6fs9whS07h9myax/37xzmgd3DDOwZZmDPCA/tGWHDA3sY\nGBxhbPzAS40FweKeTvp6OpnfXWZed4kF3R0snZcEydJ5XSxJ1y/u6WBOR9GXrcxmOIeC0dtV5tRj\ny5x6bG/LMuPjwba9ozywa5j7d+1Lg2OEh3aPsHVwhN3DFe7ZOsSOoZ1s2zt6UIAAdJYKLJrbwcI5\nHczvLtcfC+aUmT+nXF++oLvMvO4yC+d2sHhuhycZNDuCHAo2JYWC6v0Zj18+v23ZsfFg294kMAYG\nR9i6Z4Stg6PsGBplx95Rtu8dZde+CncODLJrX4Wd+yr122+bmdNRZH53mZ7OEnM7S/R2lVgwpyMN\njxI9nWV6u0r0dJbo7ijSXS7S01Vi0ZwOFvV00NtZcgvFbIocCnbYFQtiaW8XS3u7przNcGWMHUOj\n7ByqJEExVGHn0Cjb0hDZva/C4EiVwZEqu/dV2LR9iF37KuwerjZtlTQqFcS87iQ45nUlf5NHma5y\ngc5Ska5yIW2ldDB/zoFl53YmgdNZKjhcbNZzKNiM0FUuctz8bo6b3/2wtosIhivj7BmpMDhcZV9l\njH2jY+wZqbI9bZ1s3zvK7uEKe4aTQNkzXOWerUPsGa4wUh1npDrOvsrYpOFSEMzpKDGno8jczhLd\n5SJzO4vM6Sgxt7NIdzn5WwuR3q4Sczv2t1660vJzO5PlczuLzO0oUXBnvc0gDgU7qklKPnQ7iixt\n3SUyqYhgaHR/a2X3cBIye4ar7B2tsndkjL0jVYZGxxgarbJ3dIx96fKdQ6Ns2TnG0OhYWrZKZWzq\n43+SwCjQVd5/6asWKB2lAh3FAuVigTmdRXo6S8zpKNFdLtCZblcLnO5yElDdHUn4dBQLlAoFSkXR\nWSr4NmObEoeCGUm4zE37LJYffFfvwzZcGWNwJAmI4UrSEhkaScJkaLRaXzc4kiwfro4xUhlnqDLG\n4HCyfuueISpj44yOjTNaHWff6BiDo1UOdbxpV7lQ75eptVQ6S0UkKEiUizqo1VNrGXWXi3Sml9pK\nBVEsiFJavrcrKVNIL60VpCSgOoueJv4o5FAwy0BX+u19SU/nYd1vRLCvMsZwZZzhylj6GE+XjdVb\nMvtGxxgdG6c6FlTHx9k3Os7gSIXBeosnaeUMjVYJYDxIg6fWGkpaPY90woOOYmF/K6ijSFfaf9NZ\nLtJZKtBZSlpByfMkeDqKhaSFlK6rhVC9XLlAV1q2q1SkI20FldKg6k6PVWs5eSzNw+NQMDuKSEq/\nvWd/rFp/zdBoleHq/hAaH4exCCpj42lrp8rQyBhBEJEETK1lNDhaZaSStHKGqweG2J7hKtuq+1tC\no9Vxhqtj9efVSfp4pqqzVKD7gJZMEla1YCqmoVMs6IAAKhcLabkCczqK9VZWV7mhtVRQPZDKxQKF\ngig2tLpq4VQu7i/XqFxMLu+Vi4V6uem+mcGhYGZNNfbXTIex8SR4xiOojgeV9KaAkTSgan9Hq+OM\njQdj48HoWBJAQ5Wxeqtn3+gY+ypjjKfNnvGASnWc4eo4I+kNBmORbF8dS/4OVqtUxsYZqSShNTQ6\nVr+RIUvFguiaEFS1gCoXxV+evZrznnh8pnVwKJjZjJR8KM6sgYvVsXEq6SW55NLc/ue1ABsbT2Y5\nrl3KG66MHbBNTUAaQvvDrhZgtZAbi6gfc3RsnAXd5czP0aFgZjZFpWKBUhFgZoXV4eRbA8zMrM6h\nYGZmdQ4FMzOryzQUJJ0jaYOkjZIuabK+U9JX0vU/kbQyy/qYmVl7mYWCpCLwceBc4DTgQkmnTSj2\nGmBHRJwCfBj4h6zqY2Zmk8uypXAGsDEi7oqIUeDLwPkTypwPfD59/jXgOZrukRtmZjmWZSgsAzY1\nvN6cLmtaJiKqwC5g8cQdSbpIUr+k/oGBgYyqa2ZmWYZCs2/8E8etT6UMEXFpRKyJiDV9fX2HpXJm\nZnawLAevbQZWNLxeDmxpUWazpBIwH9jebqc33XTTVkn3HmKdlgBbD3Hbo1kezzuP5wz5PO88njM8\n/PM+cSqFsgyFG4FVkk4Cfg1cAPzBhDJrgT8Cfgy8FPjfiPbzMkbEITcVJPVHxJpD3f5olcfzzuM5\nQz7PO4/nDNmdd2ahEBFVSRcD15CMCb8sItZLehfQHxFrgc8CX5S0kaSFcEFW9TEzs8llOvdRRFwN\nXD1h2dsbng8DL8uyDmZmNnV5G9F86XRXYJrk8bzzeM6Qz/PO4zlDRuetSS7hm5lZjuStpWBmZm04\nFMzMrC43oTDZ5HyzgaQVkq6VdLuk9ZL+Ml2+SNJ3JN2R/l043XU93CQVJf1M0lXp65PSSRbvSCdd\nPAK/anxkSVog6WuSfpm+52fm5L1+Y/rv+xeSrpDUNdveb0mXSXpI0i8aljV9b5X4aPrZdqukJz+S\nY+ciFKY4Od9sUAXeHBGPAZ4OvCE9z0uA70bEKuC76evZ5i+B2xte/wPw4fScd5BMvjjbfAT4dkQ8\nGngiyfnP6vda0jLgL4A1EfE4ktvdL2D2vd+XA+dMWNbqvT0XWJU+LgI++UgOnItQYGqT8x31IuL+\niLg5fb6H5ENiGQdOPPh54EXTU8NsSFoO/A7wmfS1gN8mmWQRZuc5zwOeRTLWh4gYjYidzPL3OlUC\nutNZEOYA9zPL3u+IuJ6DZ3do9d6eD3whEjcACyQdd6jHzksoTGVyvlkl/W2K04GfAMdExP2QBAew\ndPpqlol/At4CjKevFwM700kWYXa+3ycDA8Dn0stmn5E0l1n+XkfEr4EPAveRhMEu4CZm//sNrd/b\nw/r5lpdQmNLEe7OFpB7g68BfRcTu6a5PliS9EHgoIm5qXNyk6Gx7v0vAk4FPRsTpwF5m2aWiZtLr\n6OcDJwHHA3NJLp9MNNve73YO67/3vITCVCbnmxUklUkC4d8i4t/TxQ/WmpPp34emq34ZeAZwnqR7\nSC4L/jZJy2FBenkBZuf7vRnYHBE/SV9/jSQkZvN7DXA2cHdEDEREBfh34DeY/e83tH5vD+vnW15C\noT45X3pXwgUkk/HNKum19M+Sn6PcAAAE+klEQVQCt0fEPzasqk08SPr3P4903bISEW+NiOURsZLk\nff3fiHgFcC3JJIswy84ZICIeADZJOjVd9BzgNmbxe526D3i6pDnpv/faec/q9zvV6r1dC7wqvQvp\n6cCu2mWmQ5GbEc2SXkDyDbI2Od97prlKh52kZwLfB37O/uvrf0vSr3AlcALJ/1Qvi4i2U5QfjSSd\nBfx1RLxQ0skkLYdFwM+AV0bEyHTW73CT9CSSzvUO4C7g1SRf9Gb1ey3p74HfJ7nb7mfAn5JcQ581\n77ekK4CzSKbHfhB4B/ANmry3aTh+jORupSHg1RHRf8jHzksomJnZ5PJy+cjMzKbAoWBmZnUOBTMz\nq3MomJlZnUPBzMzqHAo2Y0j6Ufp3paQ/OMz7/ttmx8qKpBdJevvkJQ9p3387eamHvc/HS7r8cO/X\njj6+JdVmnMbxBg9jm2JEjLVZPxgRPYejflOsz4+A8yJi6yPcz0HnldW5SPof4E8i4r7DvW87eril\nYDOGpMH06fuA35R0Szp3flHSByTdmM4X/7q0/FlKfj/iSyQD9pD0DUk3pfPtX5Quex/JrJq3SPq3\nxmOlo0A/kM7N/3NJv9+w7+u0//cK/i0dJISk90m6La3LB5ucx2pgpBYIki6X9C+Svi/pV+l8TbXf\ngJjSeTXsu9m5vFLST9Nln0qnikfSoKT3SFon6QZJx6TLX5ae7zpJ1zfs/psko8ItzyLCDz9mxAMY\nTP+eBVzVsPwi4G3p806gn2RCtLNIJoI7qaHsovRvN/ALYHHjvpsc6/eA75CMdD+GZKTocem+d5HM\nI1MAfgw8k2TE7Ab2t7IXNDmPVwMfanh9OfDtdD+rSOaq6Xo459Ws7unzx5B8mJfT158AXpU+D+B3\n0+fvbzjWz4FlE+tPMo/UN6f734Ef0/uoTSBlNpM9D3iCpNrcNvNJPlxHgZ9GxN0NZf9C0ovT5yvS\nctva7PuZwBWRXKJ5UNL3gKcCu9N9bwaQdAuwErgBGAY+I+lbwFVN9nkcybTWja6MiHHgDkl3AY9+\nmOfVynOApwA3pg2ZbvZPlDbaUL+bgOemz38IXC7pSpIJ5WoeIpl51HLMoWBHAwF/HhHXHLAw6XvY\nO+H12cCZETEk6TqSb+ST7buVxrlzxoBSRFQlnUHyYXwBcDHJzKyN9pF8wDea2HkXTPG8JiHg8xHx\n1ibrKhFRO+4Y6f/vEfF6SU8j+WGiWyQ9KSK2kfy32jfF49os5T4Fm4n2AL0Nr68B/kzJtOBIWq3k\nB2Ummg/sSAPh0SQ/SVpTqW0/wfXA76fX9/tIfs3sp60qpuS3KuZHxNXAXwFPalLsduCUCcteJqkg\n6VEkP5Cz4WGc10SN5/Jd4KWSlqb7WCTpxHYbS3pURPwkIt4ObGX/tMurSS65WY65pWAz0a1AVdI6\nkuvxHyG5dHNz2tk7QPOfW/w28HpJt5J86N7QsO5S4FZJN0cytXbNfwBnAutIvr2/JSIeSEOlmV7g\nPyV1kXxLf2OTMtcDH5Kkhm/qG4DvkfRbvD4ihiV9ZornNdEB5yLpbcB/SyoAFeANwL1ttv+ApFVp\n/b+bnjvAs4FvTeH4Nov5llSzDEj6CEmn7f+k9/9fFRFfm2SzaSOpkyS0nhn7f9bScsiXj8yy8V6S\nH5U/WpwAXOJAMLcUzMyszi0FMzOrcyiYmVmdQ8HMzOocCmZmVudQMDOzuv8Ps1R0adj7rUAAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2acd70ff240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "W1 =\n",
      "[[-0.07698201 -0.08336387 -0.07433794 ... -0.03583223  0.00852627\n",
      "   0.02142677]\n",
      " [ 0.06581955  0.0486571   0.02417413 ...  0.08421017  0.05880846\n",
      "  -0.07478838]\n",
      " [-0.03511408  0.07325806 -0.00935223 ...  0.04019384  0.06981052\n",
      "  -0.04618645]\n",
      " ...\n",
      " [-0.04202675 -0.00866584  0.07966198 ... -0.03028224 -0.08036571\n",
      "  -0.00710306]\n",
      " [-0.00700899  0.03899312 -0.0460798  ...  0.07146367  0.0320491\n",
      "   0.00994988]\n",
      " [ 0.05764115 -0.05095618  0.03379001 ...  0.05744845  0.02907361\n",
      "  -0.04694676]]\n",
      "b1 =\n",
      "[[-0.07698201 -0.08336387 -0.07433794 ... -0.03583223  0.00852627\n",
      "   0.02142677]\n",
      " [ 0.06581955  0.0486571   0.02417413 ...  0.08421017  0.05880846\n",
      "  -0.07478838]\n",
      " [-0.03511408  0.07325806 -0.00935223 ...  0.04019384  0.06981052\n",
      "  -0.04618645]\n",
      " ...\n",
      " [-0.04202675 -0.00866584  0.07966198 ... -0.03028224 -0.08036571\n",
      "  -0.00710306]\n",
      " [-0.00700899  0.03899312 -0.0460798  ...  0.07146367  0.0320491\n",
      "   0.00994988]\n",
      " [ 0.05764115 -0.05095618  0.03379001 ...  0.05744845  0.02907361\n",
      "  -0.04694676]]\n",
      "Model saved.\n",
      "Train Accuracy: 0.99145\n",
      "Test Accuracy: 0.9422\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train,Y_train, X_test, Y_test, network_layer=[784,16,16,10],learning_rate=0.0001, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存神经网络节点参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3'])\n",
      "W1\n",
      "b1\n",
      "W2\n",
      "b2\n",
      "W3\n",
      "b3\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "print(type(parameters))\n",
    "params = {}\n",
    "print(parameters.keys())\n",
    "for (k, v) in parameters.items():\n",
    "    print(k)\n",
    "    params[k] = tf.convert_to_tensor(v)\n",
    "saver = tf.train.Saver(parameters)\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "network_layer=[784,16,16,10]\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "    sess.run(init_op)\n",
    "    saver.save(sess, \"./saved_model/modeHAHA.ckpt\")\n",
    "    print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07698201 -0.08336387 -0.07433794 ... -0.03583223  0.00852627\n",
      "   0.02142677]\n",
      " [ 0.06581955  0.0486571   0.02417413 ...  0.08421017  0.05880846\n",
      "  -0.07478838]\n",
      " [-0.03511408  0.07325806 -0.00935223 ...  0.04019384  0.06981052\n",
      "  -0.04618645]\n",
      " ...\n",
      " [-0.04202675 -0.00866584  0.07966198 ... -0.03028224 -0.08036571\n",
      "  -0.00710306]\n",
      " [-0.00700899  0.03899312 -0.0460798  ...  0.07146367  0.0320491\n",
      "   0.00994988]\n",
      " [ 0.05764115 -0.05095618  0.03379001 ...  0.05744845  0.02907361\n",
      "  -0.04694676]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(parameters['W1'].eval())\n",
    "    print(parameters['b1'].eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
